<h1>AvianSight: Bird Species Classification for Amateur Birdwatchers</h1>
<h2>Overall project checklist</h2>
<p>The checklist is <em>exhaustic</em> which means that it includes everything that you could possible do on the project in
relation the curricilum in this course. Therefore, we do not expect at all that you have checked of all boxes at the
end of the project.</p>
<h3>Week 1</h3>
<ul>
<li>[x] Create a git repository</li>
<li>[x] Make sure that all team members have write access to the github repository</li>
<li>[x] Create a dedicated environment for you project to keep track of your packages</li>
<li>[x] Create the initial file structure using cookiecutter</li>
<li>[x] Fill out the <code>make_dataset.py</code> (<code>save_data_mean_std.py</code>) file such that it downloads whatever data you need</li>
<li>[x] Add a model file and a training script and get that running</li>
<li>[x] Remember to fill out the <code>requirements.txt</code> file with whatever dependencies that you are using</li>
<li>[x] Remember to comply with good coding practices (<code>pep8</code>) while doing the project</li>
<li>[x] Do a bit of code typing and remember to document essential parts of your code</li>
<li>[x] Setup version control for your data or part of your data</li>
<li>[x] Construct one or multiple docker files for your code</li>
<li>[x] Build the docker files locally and make sure they work as intended</li>
<li>[x] Write one or multiple configurations files for your experiments</li>
<li>[x] Used Hydra to load the configurations and manage your hyperparameters</li>
<li>[x] When you have something that works somewhat, remember at some point to to some profiling and see if
      you can optimize your code</li>
<li>[x] Use Weights &amp; Biases to log training progress and other important metrics/artifacts in your code. Additionally,
      consider running a hyperparameter optimization sweep.</li>
<li>[x] Use Pytorch-lightning (if applicable) to reduce the amount of boilerplate in your code</li>
</ul>
<h3>Week 2</h3>
<ul>
<li>[x] Write unit tests related to the data part of your code</li>
<li>[x] Write unit tests related to model construction and or model training</li>
<li>[x] Calculate the coverage.</li>
<li>[x] Get some continuous integration running on the github repository</li>
<li>[x] Create a data storage in GCP Bucket for you data and preferable link this with your data version control setup</li>
<li>[x] Create a trigger workflow for automatically building your docker images</li>
<li>[x] Get your model training in GCP using either the Engine or Vertex AI</li>
<li>[x] Create a FastAPI application that can do inference using your model</li>
<li>[ ] If applicable, consider deploying the model locally using torchserve</li>
<li>[x] Deploy your model in GCP using either Functions or Run as the backend</li>
</ul>
<h3>Week 3</h3>
<ul>
<li>[ ] Check how robust your model is towards data drifting</li>
<li>[x] Setup monitoring for the system telemetry of your deployed model</li>
<li>[x] Setup monitoring for the performance of your deployed model</li>
<li>[x] If applicable, play around with distributed data loading</li>
<li>[x] If applicable, play around with distributed model training</li>
<li>[ ] Play around with quantization, compilation and pruning for you trained models to increase inference speed</li>
</ul>
<h3>Additional</h3>
<ul>
<li>[x] Revisit your initial project description. Did the project turn out as you wanted?</li>
<li>[x] Make sure all group members have a understanding about all parts of the project</li>
<li>[x] Uploaded all your code to github</li>
</ul>
<h2>Group information</h2>
<h3>Question 1</h3>
<blockquote>
<p><strong>Enter the group number you signed up on learn.inside.dtu.dk</strong></p>
<p>Answer:</p>
</blockquote>
<p>This project has been executed by group: MLOps 4.</p>
<h3>Question 2</h3>
<blockquote>
<p><strong>Enter the study number for each member in the group</strong></p>
<p>Answer:</p>
</blockquote>
<p>This project has been executed by students: s184202, s193973, s222681 and s222698</p>
<h3>Question 3</h3>
<blockquote>
<p><strong>What framework did you choose to work with and did it help you complete the project?</strong></p>
<p>Answer:</p>
</blockquote>
<p>We chose to work with the TIMM (PyTorch Image Models), PyTorch Lightning and TorchMetrics frameworks; where TIMM's pre-trained models, PyTorch Lightning's training framework and TorchMetrics' performance metrics formed a robust environment for developing a bird species classifier.</p>
<p>The TIMM framework was central to our model development with its wide range of pre-trained computer vision models which allowed leveraging transfer learning in fine-tuning a classification model. The function <code>create_model</code> was used to initialise model architectures with pre-trained weights and tailor them to our dataset.</p>
<p>A training pipeline was set up with PyTorch Lightning’s LightningModule and Trainer; organising training and validation steps, model checkpoints and logging with WANDB. This enabled us to focus on the model's logic rather than the boilerplate code. TorchMetrics was similarly used to handle performance metrics, providing a convenient and reliable way to calculate accuracy to be logged during both training and validation phases.</p>
<h2>Coding environment</h2>
<blockquote>
<p>In the following section we are interested in learning more about you local development environment.</p>
</blockquote>
<h3>Question 4</h3>
<blockquote>
<p><strong>Explain how you managed dependencies in your project? Explain the process a new team member would have to go</strong>
<strong>through to get an exact copy of your environment.</strong></p>
<p>Answer:</p>
</blockquote>
<p>In this project a <code>.toml</code> file was used for managing dependencies, compatible with different environment managers like pipenv, conda, and venv. This approach allowed flexibility in tool choice while ensuring a consistent development setup.</p>
<p>For a new team member to set up their environment, the steps are as follows:</p>
<ol>
<li>
<p><strong>Clone the Repository</strong>: Start by cloning the project repository.</p>
</li>
<li>
<p><strong>Choose an Environment Manager</strong>:</p>
</li>
<li>For <strong>venv</strong>: Run <code>python -m venv env</code> and activate it with <code>source env/bin/activate</code> (Unix/macOS) or <code>env\Scripts\activate</code> (Windows).</li>
<li>For <strong>pipenv</strong>: Use <code>pipenv shell</code> to create and activate the environment.</li>
<li>
<p>For <strong>conda</strong>: Create a new environment with <code>conda create --name myenv</code> and activate it using <code>conda activate myenv</code>.</p>
</li>
<li>
<p><strong>Install the Project</strong>: Run <code>pip install -e .</code> in the project directory. This command installs the project in editable mode and also automatically installs build dependencies as specified in the <code>.toml</code> file and the dependencies listed in <code>requirements.txt</code>.</p>
</li>
<li>
<p><strong>Install Additional Development Dependencies</strong> (if applicable): For any optional development dependencies, install them from <code>requirements_dev.txt</code> using <code>pip install -r requirements_dev.txt</code>.</p>
</li>
</ol>
<p>This process ensures that the team member's development environment mirrors the project's setup, maintaining consistency across different setups.</p>
<h3>Question 5</h3>
<blockquote>
<p><strong>We expect that you initialized your project using the cookiecutter template. Explain the overall structure of your</strong>
<strong>code. Did you fill out every folder or only a subset?</strong></p>
<p>Answer:</p>
</blockquote>
<p>The project's code structure, initially set up using the provided cookiecutter template, underwent minor modifications to better suit the specific needs:</p>
<ul>
<li><strong>Source Folder (<code>src</code>)</strong>: Renamed the standard source folder to <code>src</code> for enhanced clarity.</li>
<li><strong>Config and Utils Folders</strong>: Within <code>src</code>, added a <code>config</code> folder for Hydra configurations and a <code>utils</code> folder for essential utility scripts, like index to class mappings.</li>
</ul>
<p>Apart from these additions, all other folders from the cookiecutter template were utilized, with the exception of the <code>notebooks</code> folder, which was not needed for this project. These minor changes ensure that the project structure remains organized and efficient, while being tailored to the project.</p>
<h3>Question 6</h3>
<blockquote>
<p><strong>Did you implement any rules for code quality and format? Additionally, explain with your own words why these</strong>
<strong>concepts matters in larger projects.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Sticking to PEP 8 guidelines was the key strategy for code quality and format. In large-scale projects, this approach is important. It ensures the code is clear and uniform, which simplifies reading and teamwork. Following these standards also helps in keeping up the quality of the code, minimizes errors, and makes debugging easier. Essentially, it’s about easing the team collaboration by maintaining a strong codebase in more complex projects.</p>
<h2>Version control</h2>
<blockquote>
<p>In the following section we are interested in how version control was used in your project during development to
corporate and increase the quality of your code.</p>
</blockquote>
<h3>Question 7</h3>
<blockquote>
<p><strong>How many tests did you implement and what are they testing in your code?</strong></p>
<p>Answer:</p>
</blockquote>
<p>In total, we have implemented four tests. These tests have been made to guarantee the reliability and robustness of our data 
handling and model generation components. For the data handling a single test was created, <code>test_data_loading</code>, that assures that 
the data is properly loaded. </p>
<p>For the model generation, we implemented three tests: <code>test_forward_pass</code> - which tests the forward pass of the model, 
<code>test_training_step</code> - which tests the training pass of the model, and <code>test_validation_step</code> - which tests the validation pass of<br />
the model.</p>
<h3>Question 8</h3>
<blockquote>
<p><strong>What is the total code coverage (in percentage) of your code? If you code had an code coverage of 100% (or close</strong>
<strong>to), would you still trust it to be error free? Explain you reasoning.</strong></p>
<p>Answer:</p>
</blockquote>
<p>The total code coverage of the code is 90%, as can be seen from the output of <code>coverage report</code>:
```
Name                     Stmts   Miss  Cover</p>
<hr />
<p>src/<strong>init</strong>.py              0      0   100%
src/data/<strong>init</strong>.py         0      0   100%
src/data/data.py            32      4    88%
src/models/<strong>init</strong>.py       0      0   100%
src/models/model.py         50     14    72%
tests/<strong>init</strong>.py            4      0   100%
tests/test_data.py          55      0   100%
tests/test_model.py         40      0   100%</p>
<hr />
<p>TOTAL                      181     18    90%</p>
<p>```
Even if the coverage is 10% far from the perfect one, this does not guarantee the lack of bugs and errors in the code.
Despite our extensive testing of the data loader and model training modules, unforeseen interactions and edge cases may still exist. 
Nevertheless, a high code coverage is a good indicator that the code has been tested.</p>
<h3>Question 9</h3>
<blockquote>
<p><strong>Did you workflow include using branches and pull requests? If yes, explain how. If not, explain how branches and</strong>
<strong>pull request can help improve version control.</strong></p>
<p>Answer:</p>
</blockquote>
<p>We made use of both branches and PRs in our project. For every task, we created a branch and also protected the main branch by
adding the following rules: at least one person needs to approve any PR, all your workflows have to pass and all conversations need 
to be resolved. By using branches and pull requests in version control it ensures that changes are reviewed before merging,
maintaining code integrity, and facilitating smoother project evolution.</p>
<h3>Question 10</h3>
<blockquote>
<p><strong>Did you use DVC for managing data in your project? If yes, then how did it improve your project to have version</strong>
<strong>control of your data. If no, explain a case where it would be beneficial to have version control of your data.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Using Data Version Control (DVC) in the project provided an easy and seamless method to pull data onto VM instances for training, both on DTU's High-Performance Computing (HPC) Cluster and Google Cloud Platform (GCP). This setup allowed for straightforward synchronization of the latest datasets and models from the DVC remote (initially on Google Drive, then on a GCP bucket) directly to the VMs. Whether training was conducted on the HPC Cluster or GCP, DVC ensured that the most current version of the data was always used, streamlining the workflow and enhancing the efficiency and consistency of the training process across different platforms.</p>
<h3>Question 11</h3>
<blockquote>
<p><strong>Discuss you continues integration setup. What kind of CI are you running (unittesting, linting, etc.)? Do you test</strong>
<strong>multiple operating systems, python version etc. Do you make use of caching? Feel free to insert a link to one of</strong>
<strong>your github actions workflow.</strong></p>
<p>Answer:</p>
</blockquote>
<p>We set up our Continuous Integration (CI) using two separate files: one for checking code standards, named .github/workflows/codecheck.yml, and another for running unit tests, called .github/workflows/tests.yml. To make sure our code meets standards, we use tools like Ruff and MyPy for type checking. We only tested the code on one operating system, specifically ubuntu-20.04, as it was the operating system used for developing the project, and on one version of Python, namely python 3.10.0, as required by the project.</p>
<p>To make the processes faster, we use a caching mechanism. This way, every package we download won't be deleted after the workflow finishes, improving the overall speed of the workflow. Additionally, we included a <code>requirements_tests.txt</code> file with the specific packages required for running the workflow - for example, typing packages required by mypy.</p>
<p>Our GitHub Actions workflows run automatically every time we merge a branch into the main branch or create a pull request. This helps us ensure our project's integrity and quality by consistently checking for issues and making sure everything works well.</p>
<p>An example of a triggered workflow can be seen here: <a href="https://github.com/CristianaLazar/mlops-bird-classification-project/actions/runs/7555347779/job/20570134093">https://github.com/CristianaLazar/mlops-bird-classification-project/actions/runs/7555347779/job/20570134093</a></p>
<h2>Running code and tracking experiments</h2>
<blockquote>
<p>In the following section we are interested in learning more about the experimental setup for running your code and
especially the reproducibility of your experiments.</p>
</blockquote>
<h3>Question 12</h3>
<blockquote>
<p><strong>How did you configure experiments? Did you make use of config files? Explain with coding examples of how you would</strong>
<strong>run a experiment.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Experiments are configured using Hydra with distinct YAML files for each experiment in a config group. For instance, exp1.yaml and exp2.yaml are placed in an experiment directory. To run an experiment, you specify the configuration file as a command-line argument. For example, to run exp1, the command is python train_model.py experiment=exp1. This approach replaces the need for an argparser, as Hydra handles the parsing and merging of configurations from the command line and the YAML files.</p>
<h3>Question 13</h3>
<blockquote>
<p><strong>Reproducibility of experiments are important. Related to the last question, how did you secure that no information</strong>
<strong>is lost when running experiments and that your experiments are reproducible?</strong></p>
<p>Answer:</p>
</blockquote>
<p>To secure reproducibility and minimize information loss in experiments, Hydra and PyTorch Lightning are employed. Each experiment is configured using a dedicated YAML file, providing consistent settings. When an experiment runs, Hydra creates a unique directory, storing all outputs, logs, and configurations, ensuring a comprehensive record.</p>
<p>For randomness control, PyTorch Lightning's seed_everything function is used to seed all random number generators consistently. This is crucial for experiments with stochastic processes, maintaining reproducibility. To replicate an experiment, the same configuration file and seed are used, like running python train_model.py experiment=exp1. This approach, combining Hydra's configuration management with PyTorch Lightning's seeding, guarantees precise and replicable experiment documentation.</p>
<h3>Question 14</h3>
<blockquote>
<p><strong>Upload 1 to 3 screenshots that show the experiments that you have done in W&amp;B (or another experiment tracking</strong>
<strong>service of your choice). This may include loss graphs, logged images, hyperparameter sweeps etc. Explain what metrics you are tracking</strong>
<strong>and why they are important.</strong></p>
<p>Answer:</p>
</blockquote>
<p>In the project, tracking training and validation loss, along with accuracy, both on a step-wise and per epoch basis, was essential for evaluating model performance under various conditions.</p>
<p>Logging both training and validation metrics is critical. Training metrics assess how well the model learns from the dataset, but they don't tell the whole story. Over-reliance on training data can lead to overfitting. Validation metrics, however, provide insight into the model's ability to generalize to new data, a crucial factor for real-world applicability.</p>
<p>The step-wise tracking, as depicted in the image below, offers a granular view of the model's performance on different data batches. This level of detail can uncover fluctuations in model performance that might not be apparent from epoch-wise metrics alone.</p>
<p><img alt="Step-wise Stability Analysis" src="figures/step_wise_stability.png" /></p>
<p>The experiments conducted involved testing various image resizing parameters (224 and 384) and augmentation strategies. Analyzing how these factors impacted the key metrics was key to optimizing the models for accuracy.</p>
<p>Also, the project compared different model sizes: one larger for cloud-based deployment and a smaller one for on-device applications. This differentiation was essential to envistigate weather an on-device model was viable without sacrificing too much model performance. 
Below is an image showing the effects of these varying parameters and strategies on the model's performance.</p>
<p><img alt="Training and Validation Metrics" src="figures/train_val_metrics.png" /></p>
<p>The ongoing analysis of these metrics and adaptation of parameters aimed to refine the models, ensuring they are not only accurate but also effective and reliable for practical use.</p>
<h3>Question 15</h3>
<blockquote>
<p><strong>Docker is an important tool for creating containerized applications. Explain how you used docker in your</strong>
<strong>experiments? Include how you would run your docker images and include a link to one of your docker files.</strong></p>
<p>Answer:</p>
</blockquote>
<p>As reproducibility is crucial, for this project, we developed several images: one for training, one for inference and one for deployment - to guarantee that the application can run on any device.  The following commands can be used to create and run the docker files:</p>
<p>To build the docker file into a docker image:
<code>docker build -f dockerfiles/trainer.dockerfile . -t trainer:latest
    docker build -f dockerfiles/predicter.dockerfile . -t predicter:latest</code></p>
<p>To run the docker images:
<code>docker run --name experiment1 trainer:latest
    docker run --name predict predicter:latest</code></p>
<p>To automate the process even more, we created in Google Cloud a trigger for docker image creation. Every time a branch is merged into <code>main</code>, the docker files are created by using the configurations from <code>cloudbuild.yaml</code>. Once constructed, these docker images are executed using Google Cloud. </p>
<p>A link to the training Docker file can be found <a href="https://github.com/CristianaLazar/mlops-bird-classification-project/blob/main/dockerfiles/trainer.dockerfile">here</a> </p>
<h3>Question 16</h3>
<blockquote>
<p><strong>When running into bugs while trying to run your experiments, how did you perform debugging? Additionally, did you</strong>
<strong>try to profile your code or do you think it is already perfect?</strong></p>
<p>Answer length: 100-200 words.</p>
<p>Example:
<em>Debugging method was dependent on group member. Some just used ... and others used ... . We did a single profiling</em>
<em>run of our main code at some point that showed ...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 16 fill here ---</p>
<h2>Working in the cloud</h2>
<blockquote>
<p>In the following section we would like to know more about your experience when developing in the cloud.</p>
</blockquote>
<h3>Question 17</h3>
<blockquote>
<p><strong>List all the GCP services that you made use of in your project and shortly explain what each service does?</strong></p>
<p>Answer length: 50-200 words.</p>
<p>Example:
<em>We used the following two services: Engine and Bucket. Engine is used for... and Bucket is used for...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 17 fill here ---</p>
<h3>Question 18</h3>
<blockquote>
<p><strong>The backbone of GCP is the Compute engine. Explained how you made use of this service and what type of VMs</strong>
<strong>you used?</strong></p>
<p>Answer length: 100-200 words.</p>
<p>Example:
<em>We used the compute engine to run our ... . We used instances with the following hardware: ... and we started the</em>
<em>using a custom container: ...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 18 fill here ---</p>
<h3>Question 19</h3>
<blockquote>
<p><strong>Insert 1-2 images of your GCP bucket, such that we can see what data you have stored in it.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Buckets created in the project:
<img alt="Buckets" src="figures/buckets.png" /></p>
<p>Example of structure in a data bucket:
<img alt="Content example of Bucket" src="figures/bucket_content.png" /></p>
<h3>Question 20</h3>
<blockquote>
<p><strong>Upload one image of your GCP container registry, such that we can see the different images that you have stored.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Project container registry:
<img alt="Containers" src="figures/container_reg.png" /></p>
<h3>Question 21</h3>
<blockquote>
<p><strong>Upload one image of your GCP cloud build history, so we can see the history of the images that have been build in</strong>
<strong>your project.</strong></p>
<p>Answer:</p>
</blockquote>
<p>Project's Cloud Build history:
<img alt="Containers" src="figures/cloud_build.png" /></p>
<h3>Question 22</h3>
<blockquote>
<p><strong>Did you manage to deploy your model, either in locally or cloud? If not, describe why. If yes, describe how and</strong>
<strong>preferably how you invoke your deployed service?</strong></p>
<p>Answer:</p>
</blockquote>
<p>The deployment process involved wrapping our trained model within a FastAPI application that enables users to run inference on JPG images of birds to receive the classification bird species name and certainty/probability as response. The application was then containerised to ensure consistent runs across environments by packaging the application and its dependencies into a Docker image. After verifying that the image ran as intended locally, it was pushed to the project's Cloud Registry and deployed with Cloud Run.</p>
<p>To invoke the deployed service, users can send a POST request to the inference endpoint with a JPG image, replacing [path/to/image.jpg] with the correct image path:</p>
<p><code>curl -X POST -F "bird_image=@[path/to/image.jpg]" https://aviansight-app-5gmfsq67rq-ew.a.run.app/infer_image</code></p>
<p>PS: POST a selfie and see if you discover the application's Looney Bird easter egg.</p>
<h3>Question 23</h3>
<blockquote>
<p><strong>Did you manage to implement monitoring of your deployed model? If yes, explain how it works. If not, explain how</strong>
<strong>monitoring would help the longevity of your application.</strong></p>
<p>Answer length: 100-200 words.</p>
<p>Example:
<em>We did not manage to implement monitoring. We would like to have monitoring implemented such that over time we could</em>
<em>measure ... and ... that would inform us about this ... behaviour of our application.</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 23 fill here ---</p>
<h3>Question 24</h3>
<blockquote>
<p><strong>How many credits did you end up using during the project and what service was most expensive?</strong></p>
<p>Answer length: 25-100 words.</p>
<p>Example:
<em>Group member 1 used ..., Group member 2 used ..., in total ... credits was spend during development. The service</em>
<em>costing the most was ... due to ...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 24 fill here ---</p>
<h2>Overall discussion of project</h2>
<blockquote>
<p>In the following section we would like you to think about the general structure of your project.</p>
</blockquote>
<h3>Question 25</h3>
<blockquote>
<p><strong>Include a figure that describes the overall architecture of your system and what services that you make use of.</strong>
<strong>You can take inspiration from <a href="figures/overview.png">this figure</a>. Additionally in your own words, explain the</strong>
<strong>overall steps in figure.</strong></p>
<p>Answer length: 200-400 words</p>
<p>Example:</p>
<p><em>The starting point of the diagram is our local setup, where we integrated ... and ... and ... into our code.</em>
<em>Whenever we commit code and puch to github, it auto triggers ... and ... . From there the diagram shows ...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 25 fill here ---</p>
<h3>Question 26</h3>
<blockquote>
<p><strong>Discuss the overall struggles of the project. Where did you spend most time and what did you do to overcome these</strong>
<strong>challenges?</strong></p>
<p>Answer length: 200-400 words.</p>
<p>Example:
<em>The biggest challenges in the project was using ... tool to do ... . The reason for this was ...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 26 fill here ---</p>
<h3>Question 27</h3>
<blockquote>
<p><strong>State the individual contributions of each team member. This is required information from DTU, because we need to</strong>
<strong>make sure all members contributed actively to the project</strong></p>
<p>Answer length: 50-200 words.</p>
<p>Example:
<em>Student sXXXXXX was in charge of developing of setting up the initial cookie cutter project and developing of the</em>
<em>docker containers for training our applications.</em>
<em>Student sXXXXXX was in charge of training our models in the cloud and deploying them afterwards.</em>
<em>All members contributed to code by...</em></p>
<p>Answer:</p>
</blockquote>
<p>--- question 27 fill here ---</p>